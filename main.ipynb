{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval and Web Search\n",
    "<p>\n",
    "Course Project - Clustering documents to compress inverted index<br>\n",
    "Giovanni Costa - 880892\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, multiprocessing\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "from sklearn.cluster import DBSCAN, MiniBatchKMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from utils import parse_data_files, get_tfidf_repr, TSP_solver, random_search_silhouette\n",
    "from Indexer import Indexer, EXIT_NUMBER_DOCS\n",
    "\n",
    "input_path=\"input/\"\n",
    "output_path=\"output/\"\n",
    "CORE_NUM=multiprocessing.cpu_count()\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing and TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=parse_data_files()\n",
    "print(\"Dataframe info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_docs, tf_idf_vocab=get_tfidf_repr(df.iloc[:EXIT_NUMBER_DOCS])\n",
    "print(\"TF-IDF info:\")\n",
    "print(\"Shape: \", sparse_docs.shape)\n",
    "print(\"Size in MB: {:.3f} \".format(sparse_docs.data.nbytes/ (1024**2)))\n",
    "save_npz(input_path+\"sparse_tf-idf.npz\", sparse_docs)\n",
    "with open(input_path+\"tf-idf_vocab.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tf_idf_vocab, file)\n",
    "df.to_parquet(input_path+\"df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_docs_1=load_npz(input_path+'sparse_tf-idf.npz')\n",
    "tf_idf_vocab=None\n",
    "with open(input_path+'tf-idf_vocab.pkl', 'rb') as file:\n",
    "    tf_idf_vocab=pickle.load(file)\n",
    "df=pd.read_parquet(input_path+\"df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=sparse_docs_1#[:EXIT_NUMBER_DOCS,:]\n",
    "sparse_docs=test\n",
    "#sparse_doc=sparse_docs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_svd=TruncatedSVD(n_components=100, random_state=42) #For LSA, a value of 100 is recommended.\n",
    "sparse_docs_approx=trunc_svd.fit_transform(sparse_docs)\n",
    "print(\"Current shape: \", sparse_docs_approx.shape)\n",
    "#print(\"Current density ratio:\", sparse_docs_approx.count_nonzero()/(sparse_docs_approx.shape[0]*sparse_docs_approx.shape[1]))\n",
    "#print(\"Previous shape: \", sparse_docs.shape)\n",
    "#print(\"Previous density ratio:\", sparse_docs.count_nonzero()/(sparse_docs.shape[0]*sparse_docs.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_docs=sparse_docs_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatch K-Means Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=15\n",
    "params_k_means={\"n_clusters\": [i for i in range(2, 101)]}\n",
    "k_means_obj=MiniBatchKMeans(batch_size=256*CORE_NUM, n_init=\"auto\") #For faster computations, you can set the batch_size greater than 256 * number of cores to enable parallelism on all cores\n",
    "best_k_means=random_search_silhouette(k_means_obj, sparse_docs, params_k_means, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_distances=cosine_distances(best_k_means.cluster_centers_) #kmeans.cluster_centers_[0] = centroid of cluster 0\n",
    "k_means_tsp=TSP_solver(centroid_distances)\n",
    "\n",
    "#Get the labels given to the centroid in order to get the best cluster transversal ordering\n",
    "k_means_cluster_order=k_means_tsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = pd.Series(k_means_cluster_order)\n",
    "#s[s.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_val=0\n",
    "k_means_docid_remapping={}\n",
    "for label in k_means_cluster_order:\n",
    "    indices=np.nonzero(best_k_means.labels_==label)[0]\n",
    "    dim=indices.shape[0]\n",
    "    if dim!=0: #some clusters might be empty \n",
    "        distances=cosine_distances(sparse_docs[indices], best_k_means.cluster_centers_[label].reshape(1,-1)).reshape(-1)\n",
    "        tmp_vals=dict(zip(indices[np.argsort(distances)], range(starting_val, starting_val+dim)))\n",
    "        k_means_docid_remapping.update(tmp_vals)\n",
    "        starting_val+=dim\n",
    "    else:\n",
    "        print(f\"Cluster {label} is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path+\"k_means_remapping.pkl\", \"wb\") as file:\n",
    "    pickle.dump(k_means_docid_remapping, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=15\n",
    "params_dbscan={\"min_samples\": [i for i in range(2, 21)],\n",
    "                \"eps\": [i for i in np.arange(0.05, 3.05, 0.05)]}\n",
    "dbscan_obj=DBSCAN(metric=\"cosine\")\n",
    "best_dbscan=random_search_silhouette(dbscan_obj, sparse_docs, params_dbscan, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_labels=best_dbscan.labels_[best_dbscan.core_sample_indices_]\n",
    "core_index_list=[]\n",
    "for label in np.unique(core_labels):\n",
    "    indices=np.nonzero(core_labels==label)[0]\n",
    "    label_indices=best_dbscan.core_sample_indices_[indices]\n",
    "    index=np.random.choice(label_indices)\n",
    "    core_index_list.append(index)\n",
    "core_points=sparse_docs[core_index_list] #list of core points (one that represents one cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_points_distances=cosine_distances(core_points)\n",
    "dbscan_tsp=TSP_solver(core_points_distances)\n",
    "\n",
    "#Get the labels given to the core samples (representative elements) in order to get the best cluster transversal ordering\n",
    "dbscan_cluster_order=dbscan_tsp+[-1] #add to the clusters also the outliers label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_val=0\n",
    "dbscan_docid_remapping={}\n",
    "for label in dbscan_cluster_order:\n",
    "    indices=np.nonzero(best_dbscan.labels_==label)[0] #-1 is the noise\n",
    "    dim=indices.shape[0]\n",
    "    if dim!=0: #some clusters might be empty \n",
    "        distances=cosine_distances(sparse_docs[indices], core_points[label].reshape(1,-1)).reshape(-1)\n",
    "        tmp_vals=dict(zip(indices[np.argsort(distances)], range(starting_val, starting_val+dim)))\n",
    "        dbscan_docid_remapping.update(tmp_vals)\n",
    "        starting_val+=dim\n",
    "    else:\n",
    "        print(f\"Cluster {label} is empty\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path+\"dbscan_remapping.pkl\", \"wb\") as file:\n",
    "    pickle.dump(dbscan_docid_remapping, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer=Indexer()\n",
    "inverted_index_standard=indexer.get_dict()\n",
    "k_means_inverted_index=Indexer.remap_index(inverted_index_standard, k_means_docid_remapping)\n",
    "dbscan_inverted_index=indexer.remap_index(inverted_index_standard, dbscan_docid_remapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[np.diff(v[1]) for v in inverted_index_standard.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[np.diff(v[1]) for v in k_means_inverted_index.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_inverted_index_standard = Indexer.get_total_VB_enc_size(inverted_index_standard)\n",
    "dim_k_means_inverted_index = Indexer.get_total_VB_enc_size(k_means_inverted_index)\n",
    "dim_dbscan_inverted_index = indexer.get_total_VB_enc_size(dbscan_inverted_index)\n",
    "print(f\"Standard inverted index dimension: {dim_inverted_index_standard} Bytes\")\n",
    "\n",
    "print(f\"K Means method inverted index dimension: {dim_k_means_inverted_index} Bytes ~\", end=\" \")\n",
    "print(round((dim_inverted_index_standard-dim_k_means_inverted_index)/(dim_inverted_index_standard+dim_k_means_inverted_index), 3)*100, \"% reduction\")\n",
    "\n",
    "print(f\"DBSCAN method inverted index dimension: {dim_dbscan_inverted_index} Bytes ~\", end=\" \")\n",
    "print(round((dim_inverted_index_standard-dim_dbscan_inverted_index)/(dim_inverted_index_standard+dim_dbscan_inverted_index), 3)*100, \"% reduction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
