{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval and Web Search\n",
    "<p>\n",
    "Course Project - Clustering documents to compress inverted index<br>\n",
    "Giovanni Costa - 880892\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, multiprocessing\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "from sklearn.cluster import DBSCAN, MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from utils import parse_data_files, get_tfidf_repr, TSP_solver, random_search_silhouette\n",
    "from Indexer import Indexer, EXIT_NUMBER_DOCS\n",
    "\n",
    "input_path=\"input/\"\n",
    "output_path=\"output/\"\n",
    "CORE_NUM=multiprocessing.cpu_count()\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing and TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=parse_data_files()\n",
    "print(\"Dataframe info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_docs, tf_idf_vocab=get_tfidf_repr(df)\n",
    "print(\"TF-IDF info:\")\n",
    "print(\"Shape: \", sparse_docs.shape)\n",
    "print(\"Size in MB: {:.3f} \".format(sparse_docs.data.nbytes/ (1024**2)))\n",
    "save_npz(input_path+\"sparse_tf-idf.npz\", sparse_docs)\n",
    "with open(input_path+\"tf-idf_vocab.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tf_idf_vocab, file)\n",
    "df.to_parquet(input_path+\"df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_docs_1=load_npz(input_path+'sparse_tf-idf.npz')\n",
    "tf_idf_vocab=None\n",
    "with open(input_path+'tf-idf_vocab.pkl', 'rb') as file:\n",
    "    tf_idf_vocab=pickle.load(file)\n",
    "df=pd.read_parquet(input_path+\"df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=sparse_docs_1[:EXIT_NUMBER_DOCS,:]\n",
    "sparse_docs=test\n",
    "#sparse_doc=sparse_docs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sr_proj=SparseRandomProjection(eps=0.1, random_state=42)\n",
    "sparse_docs_approx=sr_proj.fit_transform(sparse_docs)\n",
    "print(\"Current shape: \", sparse_docs_approx.shape)\n",
    "print(\"Current density ratio:\", sparse_docs_approx.count_nonzero()/(sparse_docs_approx.shape[0]*sparse_docs_approx.shape[1]))\n",
    "print(\"Previous shape: \", sparse_docs.shape)\n",
    "print(\"Previous density ratio:\", sparse_docs.count_nonzero()/(sparse_docs.shape[0]*sparse_docs.shape[1])) \"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_docs=sparse_docs_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatch K-Means Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:43<00:00,  6.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_clusters': 97}\n",
      "Best score: 0.08549902\n"
     ]
    }
   ],
   "source": [
    "n_iter=15\n",
    "params_k_means={\"n_clusters\": [i for i in range(2, 101)]}\n",
    "k_means_obj=MiniBatchKMeans(batch_size=256*CORE_NUM, n_init=\"auto\") #For faster computations, you can set the batch_size greater than 256 * number of cores to enable parallelism on all cores\n",
    "best_k_means=random_search_silhouette(k_means_obj, sparse_docs, params_k_means, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_distances=cosine_distances(best_k_means.cluster_centers_) #kmeans.cluster_centers_[0] = centroid of cluster 0\n",
    "k_means_tsp=TSP_solver(centroid_distances)\n",
    "\n",
    "#Get the labels given to the centroid in order to get the best cluster transversal ordering\n",
    "k_means_cluster_order=k_means_tsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = pd.Series(k_means_cluster_order)\n",
    "#s[s.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' indices=np.nonzero(best_k_means.labels_==2)[0]\\n#doc_ids=df.iloc[indices][\"doc_id\"]\\n#dim=doc_ids.shape[0]\\ndim=indices.shape[0]\\ndistances=cosine_distances(sparse_docs[indices], best_k_means.cluster_centers_[label].reshape(1,-1)).reshape(-1)\\ntmp_vals=dict(zip(np.argsort(distances), range(starting_val, starting_val+dim))) '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" indices=np.nonzero(best_k_means.labels_==2)[0]\n",
    "#doc_ids=df.iloc[indices][\"doc_id\"]\n",
    "#dim=doc_ids.shape[0]\n",
    "dim=indices.shape[0]\n",
    "distances=cosine_distances(sparse_docs[indices], best_k_means.cluster_centers_[label].reshape(1,-1)).reshape(-1)\n",
    "tmp_vals=dict(zip(np.argsort(distances), range(starting_val, starting_val+dim))) \"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_val=0\n",
    "k_means_docid_remapping={}\n",
    "for label in k_means_cluster_order:\n",
    "    indices=np.nonzero(best_k_means.labels_==label)[0]\n",
    "    dim=indices.shape[0]\n",
    "    if dim!=0: #some clusters might be empty \n",
    "        distances=cosine_distances(sparse_docs[indices], best_k_means.cluster_centers_[label].reshape(1,-1)).reshape(-1)\n",
    "        tmp_vals=dict(zip(indices[np.argsort(distances)], range(starting_val, starting_val+dim)))\n",
    "        k_means_docid_remapping.update(tmp_vals)\n",
    "        starting_val+=dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path+\"k_means_remapping.pkl\", \"wb\") as file:\n",
    "    pickle.dump(k_means_docid_remapping, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:06<01:26,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 3, 'eps': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:14<01:34,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 18, 'eps': 1.7500000000000002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:20<01:19,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 19, 'eps': 1.6500000000000001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:32<01:04,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 19, 'eps': 1.5000000000000002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:39<00:56,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 18, 'eps': 2.1999999999999997}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:45<00:50,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 2, 'eps': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:51<00:43,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 2, 'eps': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:57<00:37,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 8, 'eps': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [01:03<00:30,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 13, 'eps': 1.2000000000000002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [01:09<00:24,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 2, 'eps': 1.6500000000000001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [01:16<00:18,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 17, 'eps': 1.7500000000000002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [01:22<00:12,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 4, 'eps': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [01:28<00:06,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 15, 'eps': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:34<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of clusters is 1. Skipping this model. Sampled parameters used:  {'min_samples': 4, 'eps': 1.1}\n",
      "Best parameters: {'min_samples': 5, 'eps': 0.5}\n",
      "Best score: -0.01783173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_iter=15\n",
    "params_dbscan={\"min_samples\": [i for i in range(2, 21)],\n",
    "                \"eps\": [i for i in np.arange(0.05, 3.05, 0.05)]}\n",
    "dbscan_obj=DBSCAN(metric=\"cosine\")\n",
    "best_dbscan=random_search_silhouette(dbscan_obj, sparse_docs, params_dbscan, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_labels=best_dbscan.labels_[best_dbscan.core_sample_indices_]\n",
    "core_index_list=[]\n",
    "for label in np.unique(core_labels):\n",
    "    indices=np.nonzero(core_labels==label)[0]\n",
    "    label_indices=best_dbscan.core_sample_indices_[indices]\n",
    "    index=np.random.choice(label_indices)\n",
    "    core_index_list.append(index)\n",
    "core_points=sparse_docs[core_index_list] #list of core points (one that represents one cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_points_distances=cosine_distances(core_points)\n",
    "dbscan_tsp=TSP_solver(core_points_distances)\n",
    "\n",
    "#Get the labels given to the core samples (representative elements) in order to get the best cluster transversal ordering\n",
    "dbscan_cluster_order=dbscan_tsp+[-1] #add to the clusters also the outliers label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_val=0\n",
    "dbscan_docid_remapping={}\n",
    "for label in dbscan_cluster_order:\n",
    "    indices=np.nonzero(best_dbscan.labels_==label)[0] #-1 is the noise\n",
    "    dim=indices.shape[0]\n",
    "    if dim!=0: #some clusters might be empty \n",
    "        distances=cosine_distances(sparse_docs[indices], core_points[label].reshape(1,-1)).reshape(-1)\n",
    "        tmp_vals=dict(zip(indices[np.argsort(distances)], range(starting_val, starting_val+dim)))\n",
    "        dbscan_docid_remapping.update(tmp_vals)\n",
    "        starting_val+=dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path+\"dbscan_remapping.pkl\", \"wb\") as file:\n",
    "    pickle.dump(dbscan_docid_remapping, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:02<00:02,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of terms (Voc. size): 35459\n",
      "Total no. of tokens: 998681\n",
      "Total no. of documents: 2377\n",
      "Total no. of postings: 211806\n"
     ]
    }
   ],
   "source": [
    "indexer=Indexer()\n",
    "inverted_index_standard=indexer.get_dict()\n",
    "k_means_inverted_index=Indexer.remap_index(inverted_index_standard, k_means_docid_remapping)\n",
    "dbscan_inverted_index=indexer.remap_index(inverted_index_standard, dbscan_docid_remapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[np.diff(v[1]) for v in inverted_index_standard.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[np.diff(v[1]) for v in k_means_inverted_index.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard inverted index dimension: 7329976.0 Bytes\n",
      "K Means method inverted index dimension: 7455784.0 Bytes ~ -0.8999999999999999 % reduction\n",
      "DBSCAN method inverted index dimension: 7489528.0 Bytes ~ -1.0999999999999999 % reduction\n"
     ]
    }
   ],
   "source": [
    "dim_inverted_index_standard = Indexer.get_total_VB_enc_size(inverted_index_standard)\n",
    "dim_k_means_inverted_index = Indexer.get_total_VB_enc_size(k_means_inverted_index)\n",
    "dim_dbscan_inverted_index = indexer.get_total_VB_enc_size(dbscan_inverted_index)\n",
    "print(f\"Standard inverted index dimension: {dim_inverted_index_standard} Bytes\")\n",
    "\n",
    "print(f\"K Means method inverted index dimension: {dim_k_means_inverted_index} Bytes ~\", end=\" \")\n",
    "print(round((dim_inverted_index_standard-dim_k_means_inverted_index)/(dim_inverted_index_standard+dim_k_means_inverted_index), 3)*100, \"% reduction\")\n",
    "\n",
    "print(f\"DBSCAN method inverted index dimension: {dim_dbscan_inverted_index} Bytes ~\", end=\" \")\n",
    "print(round((dim_inverted_index_standard-dim_dbscan_inverted_index)/(dim_inverted_index_standard+dim_dbscan_inverted_index), 3)*100, \"% reduction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
