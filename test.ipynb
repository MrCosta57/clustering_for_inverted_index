{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval and Web Search\n",
    "<p>\n",
    "Course Project - Clustering documents to compress inverted index<br>\n",
    "Giovanni Costa - 880892\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, multiprocessing\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "from sklearn.cluster import DBSCAN, MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from utils import parse_data_files, get_tfidf_repr, TSP_solver, random_search_silhouette\n",
    "\n",
    "input_path=\"input/\"\n",
    "output_path=\"output/\"\n",
    "CORE_NUM=multiprocessing.cpu_count()\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing and TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=parse_data_files()\n",
    "print(\"Dataframe info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_docs, tf_idf_vocab=get_tfidf_repr(df)\n",
    "print(\"TF-IDF info:\")\n",
    "print(\"Shape: \", sparse_docs.shape)\n",
    "print(\"Size in MB: {:.3f} \".format(sparse_docs.data.nbytes/ (1024**2)))\n",
    "save_npz(input_path+\"sparse_tf-idf.npz\", sparse_docs)\n",
    "with open(input_path+\"tf-idf_vocab.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tf_idf_vocab, file)\n",
    "df.to_parquet(input_path+\"df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_docs_1=load_npz(input_path+'sparse_tf-idf.npz')\n",
    "tf_idf_vocab=None\n",
    "with open(input_path+'tf-idf_vocab.pkl', 'rb') as file:\n",
    "    tf_idf_vocab=pickle.load(file)\n",
    "df=pd.read_parquet(input_path+\"df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=sparse_docs_1[:100,:]\n",
    "sparse_docs=test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatch K-Means Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=10\n",
    "params_k_means={\"n_clusters\": [i for i in range(2, 36)]}\n",
    "k_means_obj=MiniBatchKMeans(batch_size=256*CORE_NUM, n_init=\"auto\") #For faster computations, you can set the batch_size greater than 256 * number of cores to enable parallelism on all cores\n",
    "best_k_means=random_search_silhouette(k_means_obj, sparse_docs, params_k_means, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances=cosine_distances(best_k_means.cluster_centers_, sparse_docs)\n",
    "min_indices=np.argmin(distances, axis=1)\n",
    "medoids=sparse_docs[min_indices]\n",
    "medoids_distances=cosine_distances(medoids)\n",
    "k_means_tsp=TSP_solver(medoids_distances)\n",
    "\n",
    "#Get the labels given to the medoids (representative elements) in order to get the best cluster transversal ordering\n",
    "k_means_cluster_order=best_k_means.labels_[min_indices[k_means_tsp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_val=0\n",
    "docid_remaping={}\n",
    "for label in k_means_cluster_order:\n",
    "    indices=np.nonzero(best_k_means.labels_==label) #-1 is the noise\n",
    "    doc_ids=df.iloc[indices][\"doc_id\"]\n",
    "    dim=doc_ids.shape[0]\n",
    "    tmp_vals=dict(zip(doc_ids, range(starting_val, starting_val+dim)))\n",
    "    docid_remaping.update(tmp_vals)\n",
    "    starting_val+=dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path+\"k_means_remapping.pkl\", \"wb\") as file:\n",
    "    pickle.dump(docid_remaping, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=10\n",
    "params_dbscan={\"min_samples\": [i for i in range(2, 36)],\n",
    "                \"eps\": [i for i in np.arange(0.1, 5.1, 0.1)]}\n",
    "dbscan_obj=DBSCAN(metric=\"cosine\")\n",
    "best_dbscan=random_search_silhouette(dbscan_obj, sparse_docs, params_dbscan, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_indices=best_dbscan.core_sample_indices_\n",
    "core_points=sparse_docs[core_indices]\n",
    "core_points_distances=cosine_distances(core_points)\n",
    "dbscan_tsp=TSP_solver(core_points_distances)\n",
    "\n",
    "#Get the labels given to the core samples (representative elements) in order to get the best cluster transversal ordering\n",
    "dbscan_cluster_order=best_dbscan.labels_[core_indices[dbscan_tsp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_val=0\n",
    "docid_remaping={}\n",
    "for label in dbscan_cluster_order:\n",
    "    indices=np.nonzero(best_dbscan.labels_==label) #-1 is the noise\n",
    "    doc_ids=df.iloc[indices][\"doc_id\"]\n",
    "    dim=doc_ids.shape[0]\n",
    "    tmp_vals=dict(zip(doc_ids, range(starting_val, starting_val+dim)))\n",
    "    docid_remaping.update(tmp_vals)\n",
    "    starting_val+=dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path+\"dbscan_remapping.pkl\", \"wb\") as file:\n",
    "    pickle.dump(docid_remaping, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
